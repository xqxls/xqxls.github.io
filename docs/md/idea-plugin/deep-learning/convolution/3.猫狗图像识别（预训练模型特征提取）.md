
## 任务描述
> 给定一个图像，识别是猫还是狗 

![1683643703490](3.猫狗图像识别(预训练模型特征提取).assets/1683643703490.png)

## 数据集介绍

> 猫狗分类数据集由 Kaggle 提供，在 2013 年底作为一项计算机视觉竞赛的一部分，当时卷积神经网络还不是主流算法。可以从 Kaggle 网站下载原始数据集 Dogs vs. Cats。
>
> 这个数据集包含 25 000 张猫和狗的图像（每个类别各有 12 500 张），压缩后的大小为 543 MB。



## 步骤一（下载数据集）

```
!curl https://public.dm.files.1drv.com/y4mkTAKHSDM6HwQUO9uV0STl5pFycoRZJ8xbFxNF_UoJelVVzYkRAxGnAR0NAXgnUJJnMoSXfevbyX1ShKWYSdqG6LkN53JdUV82FsHoVEMOujpTclhWuihAtlISFwvp9DS4axNUwArGPcdn1KgZpqnP7o6CTDPUUfkPlT-gWrd74vaLt40scE6YTuG2WDdRswFlrlArWcAgjZDocGGDWbIYzvw8yI2jvVp287nrAdbYig?AVOverride=1 --output /content/train.zip
```

首先从kaggle下载Dogs vs. Cats数据集，下载下来是一个train.zip格式的文件。由于谷歌的Colab上传文件太慢，可以先上传到oneDrive平台，再通过curl从平台下载下来。

## 步骤二（解压并提取数据）

```
!unzip -qq train.zip
```

解压之后多了一个train目录

```
import os, shutil, pathlib
original_dir = pathlib.Path("train")   
new_base_dir = pathlib.Path("cats_vs_dogs_small")   

def make_subset(subset_name, start_index, end_index):   
    for category in ("cat", "dog"):
        dir = new_base_dir / subset_name / category
        os.makedirs(dir)
        fnames = [f"{category}.{i}.jpg"
                  for i in range(start_index, end_index)]
        for fname in fnames:
            shutil.copyfile(src=original_dir / fname,
                            dst=dir / fname)

make_subset("train", start_index=0, end_index=1000)   
make_subset("validation", start_index=1000, end_index=1500)   
make_subset("test", start_index=1500, end_index=2500) 
```

下载数据并解压后，我们将创建一个新数据集，其中包含 3 个子集：训练集，每个类别各 1000 个样本；验证集，每个类别各 500 个样本；测试集，每个类别各 1000 个样本。我们通过调用几次 shutil 来创建这个子数据集。

发现多了一个cats_vs_dogs_small目录

train是用每个类别的前 1000 张图像创建的训练子集，test是 用每个类别接下来的 500 张图像创建验证子集 ，validation是用每个类别接下来的 1000 张图像创建测试子集 。

现在我们有 2000 张训练图像、1000 张验证图像和 2000 张测试图像。在这 3 个集合中，两个类别的样本数相同，所以这是一个均衡的二分类问题，分类精度可作为衡量成功的指标。

## 步骤三（数据预处理）

```
from tensorflow.keras.utils import image_dataset_from_directory
train_dataset = image_dataset_from_directory(
    new_base_dir / "train",
    image_size=(180, 180),
    batch_size=32)
validation_dataset = image_dataset_from_directory(
    new_base_dir / "validation",
    image_size=(180, 180),
    batch_size=32)
test_dataset = image_dataset_from_directory(
    new_base_dir / "test",
    image_size=(180, 180),
    batch_size=32)
```

将数据输入模型之前，应该将数据格式化为经过预处理的浮点数张量。当前数据以 JPEG 文件的形式存储在硬盘上，所以数据预处理步骤大致如下。

- 读取 JPEG 文件。

- 将 JPEG 文件解码为 RGB 像素网格。

- 将这些像素网格转换为浮点数张量。

- 将这些张量调节为相同大小（本例为 180×180）。

- 将数据打包成批量（一个批量包含 32 张图像）。

Keras 拥有自动完成这些步骤的工具。具体地说，Keras 包含实用函数 image_dataset_from_directory()，它可以快速建立数据管道，自动将磁盘上的图像文件转换为预处理好的张量批量。

调用 image_dataset_from_directory(directory)，首先会列出 directory 的子目录，并假定每个子目录都包含某一个类别的图像。然后，它会为每个子目录下的图像文件建立索引。最后，它会创建并返回一个 tf.data.Dataset 对象，用于读取这些文件、打乱其顺序、将其调节为相同大小并打包成批量。

## 步骤四（将 VGG16 卷积基实例化 ）

```
from tensorflow import keras
conv_base = keras.applications.vgg16.VGG16(
    weights="imagenet",
    include_top=False,
    input_shape=(180, 180, 3))
```

这里向构造函数传入了 3 个参数：

- weights 指定模型初始化的权重检查点。

- include_top 指定是否包含密集连接分类器。默认情况下，这个密集连接分类器对应 ImageNet 的 1000 个类别。由于我们打算使用自己的密集连接分类器（只有 cat 和 dog 这两个类别），因此不需要包含它。
- input_shape 是输入模型的图像张量的形状。这个参数完全是可选的。如果不传入这个参数，那么模型能够处理任意形状的输入。这里我们传入这个参数，以便直观地观察特征图的尺寸如何随着每个新的卷积层和汇聚层而减小。

接下来有以下两种方法可供选择：

- 在我们的数据集上运行卷积基，将输出保存为 NumPy 数组，并保存在硬盘上，然后将这个数组输入到一个独立的密集连接分类器中。这种方法速度快，计算代价低，因为对于每张输入图像只需运行一次卷积基，而卷积基是当前流程中计算代价最高的。但出于同样的原因，这种方法无法使用数据增强。

- 在已有模型（conv_base）上添加 Dense 层，并在输入数据上端到端地运行整个模型。这样就可以使用数据增强，因为每张输入图像进入模型时都会经过卷积基。但出于同样的原因，这种方法的计算代价比第一种要高很多。

## 步骤五（特征提取）

```
import numpy as np

def get_features_and_labels(dataset):
    all_features = []
    all_labels = []
    for images, labels in dataset:
        preprocessed_images = keras.applications.vgg16.preprocess_input(images)
        features = conv_base.predict(preprocessed_images)
        all_features.append(features)
        all_labels.append(labels)
    return np.concatenate(all_features), np.concatenate(all_labels)

train_features, train_labels = get_features_and_labels(train_dataset)
val_features, val_labels = get_features_and_labels(validation_dataset)
test_features, test_labels = get_features_and_labels(test_dataset)
```

predict() 只接收图像作为输入，不接收标签，但当前数据集生成的批量既包含图像又包含标签。此外，VGG16 模型的输入需要先使用函数 keras.applications.vgg16.preprocess_input 进行预处理。这个函数的作用是将像素值缩放到合适的范围内。

## 步骤六（训练模型）

```
from tensorflow import keras
from tensorflow.keras import layers

inputs = keras.Input(shape=(5, 5, 512))
x = layers.Flatten()(inputs)   
x = layers.Dense(256)(x)
x = layers.Dropout(0.5)(x)
outputs = layers.Dense(1, activation="sigmoid")(x)
model = keras.Model(inputs, outputs)
model.compile(loss="binary_crossentropy",
              optimizer="rmsprop",
              metrics=["accuracy"])
callbacks = [
    keras.callbacks.ModelCheckpoint(
        filepath="feature_extraction.keras",
        save_best_only=True,
        monitor="val_loss")
]
history = model.fit(
    train_features, train_labels,
    epochs=20,
    validation_data=(val_features, val_labels),
    callbacks=callbacks)
```

训练速度非常快，因为只需要处理两个 Dense 层。

## 步骤七（绘制精度曲线和损失曲线）

```
import matplotlib.pyplot as plt
acc = history.history["accuracy"]
val_acc = history.history["val_accuracy"]
loss = history.history["loss"]
val_loss = history.history["val_loss"]
epochs = range(1, len(acc) + 1)
plt.plot(epochs, acc, "bo", label="Training accuracy")
plt.plot(epochs, val_acc, "b", label="Validation accuracy")
plt.title("Training and validation accuracy")
plt.legend()
plt.figure()
plt.plot(epochs, loss, "bo", label="Training loss")
plt.plot(epochs, val_loss, "b", label="Validation loss")
plt.title("Training and validation loss")
plt.legend()
plt.show()
```

![1683643780934](3.猫狗图像识别(预训练模型特征提取).assets/1683643780934.png)

![1683643794322](3.猫狗图像识别(预训练模型特征提取).assets/1683643794322.png)

验证精度达到约 97%，比从头开始训练的小模型要高得多。但这种对比有失公平，因为 ImageNet 包含许多狗和猫的样本，也就是说，我们的预训练模型已经拥有完成当前任务所需的知识。在使用预训练的特征时，情况并非总是如此。然而，从图中也可以看出，尽管 dropout 比率很大，但模型几乎从一开始就出现过拟合。这是因为这种方法没有使用数据增强，而数据增强对防止小型图像数据集的过拟合非常重要。

## 步骤八（使用数据增强的特征提取）

将 VGG16 卷积基实例化并冻结 

```
conv_base = keras.applications.vgg16.VGG16(
    weights="imagenet",
    include_top=False)
conv_base.trainable = False
```

现在我们可以创建一个新模型，将以下三部分连接起来：

-  一个数据增强代码块

- 冻结的卷积基

- 一个密集连接分类器

```
data_augmentation = keras.Sequential(
    [
        layers.RandomFlip("horizontal"),
        layers.RandomRotation(0.1),
        layers.RandomZoom(0.2),
    ]
)

inputs = keras.Input(shape=(180, 180, 3))
x = data_augmentation(inputs)   
x = keras.applications.vgg16.preprocess_input(x)   
x = conv_base(x)
x = layers.Flatten()(x)
x = layers.Dense(256)(x)
x = layers.Dropout(0.5)(x)
outputs = layers.Dense(1, activation="sigmoid")(x)
model = keras.Model(inputs, outputs)
model.compile(loss="binary_crossentropy",
    optimizer="rmsprop",
    metrics=["accuracy"])

```

## 步骤九（重新训练模型）

```
callbacks = [
    keras.callbacks.ModelCheckpoint(
        filepath="feature_extraction_with_data_augmentation.keras",
        save_best_only=True,
        monitor="val_loss")
]
history = model.fit(
    train_dataset,
    epochs=50,
    validation_data=validation_dataset,
    callbacks=callbacks)
```

![1683644149924](3.猫狗图像识别(预训练模型特征提取).assets/1683644149924.png)

## 步骤十（绘制精度曲线和损失曲线）

